{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 1: Combining CVS Files\n",
        "\n",
        "## Objective\n",
        "Merge the two raw CSV files (**ai_job_dataset.csv** and **ai_job_dataset.csv**) into a single cosnolidated dataset, while handling differences in columns.\n",
        "\n",
        "## Dateset\n",
        "- **ai_job_dataset.csv** - Part 1 (19 columns, no `salary_local`)\n",
        "- **ai_job_dataset1.csv** - Part 2 (20 columns, includes `salary_local`)\n",
        "\n",
        "##  Column Differences\n",
        "- The second file includes an extra column: `salary_local`.\n",
        "\n",
        "##  Process\n",
        "1. Load both CSV files with **pandas**.\n",
        "2. Align their columns by adding any missing columns (like `salary_local`) with `NaN` values in the first dataset.\n",
        "3. Concatenate the datasets into one.\n",
        "4. Save the combined dataset as `ai_jobs_combined.csv`.\n",
        "\n",
        "##  Expected Output\n",
        "- A single CSV file with **all rows from both parts** and **consistent columns**.\n",
        "- Missing values for `salary_local` in the first file are represented as `NaN` for later cleaning in ETL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Combined dataset saved. Total rows: 30000\n",
            " Final columns: ['job_id', 'job_title', 'salary_usd', 'salary_currency', 'salary_local', 'experience_level', 'employment_type', 'company_location', 'company_size', 'employee_residence', 'remote_ratio', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'application_deadline', 'job_description_length', 'benefits_score', 'company_name']\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df1 = pd.read_csv(\"../data/inputs/raw/ai_job_dataset.csv\")\n",
        "df2 = pd.read_csv(\"../data/inputs/raw/ai_job_dataset1.csv\")\n",
        "\n",
        "# Align the columns of the two dataframes\n",
        "for col in df2.columns:\n",
        "    if col not in df1.columns:\n",
        "        df1[col] = pd.NA # Fill missing columns with NA\n",
        "\n",
        "# Ensure both datasets have same column order\n",
        "df1 = df1[df2.columns]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Save the combined dataset to a new CSV file\n",
        "combined_df.to_csv(\"../data/inputs/raw/ai_job_dataset_combined.csv\", index=False)\n",
        "\n",
        "# Print the shape of the combined dataset\n",
        "print(f\" Combined dataset saved. Total rows: {combined_df.shape[0]}\")\n",
        "print(f\" Final columns: {combined_df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 2: Handling Missing Values\n",
        "\n",
        "## Objective\n",
        "Identify and handle missing values in the combined dataset (**ai_job_dataset_combined.csv**) to ensure clean and reliable data for analysis.\n",
        "\n",
        "## What We'll Do\n",
        "1. **Load** the combined dataset.\n",
        "2. **Check** for missing values using `isnull()` and `sum()`.\n",
        "3. **Categorize missing values**:\n",
        "   - **Critical columns** (e.g., `job_id`, `job_title`, `salary_usd`) – cannot have missing data.\n",
        "   - **Optional columns** (e.g., `salary_local`, `benefits_score`) – can be missing and filled logically.\n",
        "4. **Decide on handling strategy**:\n",
        "   - **Drop rows** if critical data is missing.\n",
        "   - **Fill NaNs** for non-critical data (e.g., replace missing `salary_local` with `Not Provided` or median).\n",
        "5. **Save** the cleaned dataset as `ai_jobs_cleaned.csv`.\n",
        "\n",
        "##  Expected Output\n",
        "- Clean dataset with **no missing critical values** and logical handling of optional ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the combined dataset\n",
        "df = pd.read_csv('../data/inputs/raw/ai_job_dataset_combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "job_id                        0\n",
              "job_title                     0\n",
              "salary_usd                    0\n",
              "salary_currency               0\n",
              "salary_local              15000\n",
              "experience_level              0\n",
              "employment_type               0\n",
              "company_location              0\n",
              "company_size                  0\n",
              "employee_residence            0\n",
              "remote_ratio                  0\n",
              "required_skills               0\n",
              "education_required            0\n",
              "years_experience              0\n",
              "industry                      0\n",
              "posting_date                  0\n",
              "application_deadline          0\n",
              "job_description_length        0\n",
              "benefits_score                0\n",
              "company_name                  0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()  # Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with missing critical values\n",
        "critical_cols = [\"job_id\", \"job_title\", \"salary_usd\"]\n",
        "df = df.dropna(subset=critical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill missing values  for optional columns\n",
        "df['salary_local'] = df['salary_local'].fillna('Not Provided')\n",
        "df['benefits_score'] = df['benefits_score'].fillna(df['benefits_score'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling Missing Values – Final Summary\n",
        "\n",
        " **Findings**\n",
        "- Only one column had missing values: `salary_local` (15,000 rows).\n",
        "- All other 19 columns had **0 missing values**.\n",
        "\n",
        " **Action Taken**\n",
        "- We filled missing `salary_local` values with `\"Not Provided\"`.\n",
        "- No rows were dropped since critical columns (`job_id`, `job_title`, `salary_usd`) had no missing data.\n",
        "\n",
        " **Result**\n",
        "- Total rows: **30,000**\n",
        "- Total columns: **20**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
