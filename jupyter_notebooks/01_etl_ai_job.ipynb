{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 1: Combining CVS Files\n",
        "\n",
        "## Objective\n",
        "Merge the two raw CSV files (**ai_job_dataset.csv** and **ai_job_dataset.csv**) into a single cosnolidated dataset, while handling differences in columns.\n",
        "\n",
        "## Dateset\n",
        "- **ai_job_dataset.csv** - Part 1 (19 columns, no `salary_local`)\n",
        "- **ai_job_dataset1.csv** - Part 2 (20 columns, includes `salary_local`)\n",
        "\n",
        "##  Column Differences\n",
        "- The second file includes an extra column: `salary_local`.\n",
        "\n",
        "##  Process\n",
        "1. Load both CSV files with **pandas**.\n",
        "2. Align their columns by adding any missing columns (like `salary_local`) with `NaN` values in the first dataset.\n",
        "3. Concatenate the datasets into one.\n",
        "4. Save the combined dataset as `ai_jobs_combined.csv`.\n",
        "\n",
        "##  Expected Output\n",
        "- A single CSV file with **all rows from both parts** and **consistent columns**.\n",
        "- Missing values for `salary_local` in the first file are represented as `NaN` for later cleaning in ETL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/anita/Documents/vscode-projects/global-ai-job-markets-and-salary-trends-2025/jupyter_notebooks'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os \n",
        "current_dir =os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Combined dataset saved. Total rows: 30000\n",
            " Final columns: ['job_id', 'job_title', 'salary_usd', 'salary_currency', 'salary_local', 'experience_level', 'employment_type', 'company_location', 'company_size', 'employee_residence', 'remote_ratio', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'application_deadline', 'job_description_length', 'benefits_score', 'company_name']\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df1 = pd.read_csv(\"../data/inputs/raw/ai_job_dataset.csv\")\n",
        "df2 = pd.read_csv(\"../data/inputs/raw/ai_job_dataset1.csv\")\n",
        "\n",
        "# Align the columns of the two dataframes\n",
        "for col in df2.columns:\n",
        "    if col not in df1.columns:\n",
        "        df1[col] = pd.NA # Fill missing columns with NA\n",
        "\n",
        "# Ensure both datasets have same column order\n",
        "df1 = df1[df2.columns]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Save the combined dataset to a new CSV file\n",
        "combined_df.to_csv(\"../data/inputs/raw/ai_job_dataset_combined.csv\", index=False)\n",
        "\n",
        "# Print the shape of the combined dataset\n",
        "print(f\" Combined dataset saved. Total rows: {combined_df.shape[0]}\")\n",
        "print(f\" Final columns: {combined_df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 2: Handling Missing Values\n",
        "\n",
        "## Objective\n",
        "Identify and handle missing values in the combined dataset (**ai_job_dataset_combined.csv**) to ensure clean and reliable data for analysis.\n",
        "\n",
        "## What We'll Do\n",
        "1. **Load** the combined dataset.\n",
        "2. **Check** for missing values using `isnull()` and `sum()`.\n",
        "3. **Categorize missing values**:\n",
        "   - **Critical columns** (e.g., `job_id`, `job_title`, `salary_usd`) – cannot have missing data.\n",
        "   - **Optional columns** (e.g., `salary_local`, `benefits_score`) – can be missing and filled logically.\n",
        "4. **Decide on handling strategy**:\n",
        "   - **Drop rows** if critical data is missing.\n",
        "   - **Fill NaNs** for non-critical data (e.g., replace missing `salary_local` with `Not Provided` or median).\n",
        "5. **Save** the cleaned dataset as `ai_jobs_cleaned.csv`.\n",
        "\n",
        "##  Expected Output\n",
        "- Clean dataset with **no missing critical values** and logical handling of optional ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the combined dataset\n",
        "df = pd.read_csv('../data/inputs/raw/ai_job_dataset_combined.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "job_id                        0\n",
              "job_title                     0\n",
              "salary_usd                    0\n",
              "salary_currency               0\n",
              "salary_local              15000\n",
              "experience_level              0\n",
              "employment_type               0\n",
              "company_location              0\n",
              "company_size                  0\n",
              "employee_residence            0\n",
              "remote_ratio                  0\n",
              "required_skills               0\n",
              "education_required            0\n",
              "years_experience              0\n",
              "industry                      0\n",
              "posting_date                  0\n",
              "application_deadline          0\n",
              "job_description_length        0\n",
              "benefits_score                0\n",
              "company_name                  0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()  # Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with missing critical values\n",
        "critical_cols = [\"job_id\", \"job_title\", \"salary_usd\"]\n",
        "df = df.dropna(subset=critical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No rows dropped. Critical columns have no missing values.\n"
          ]
        }
      ],
      "source": [
        "# Define critical columns for analysis\n",
        "critical_cols = [\"job_id\", \"job_title\", \"salary_usd\"]\n",
        "\n",
        "# Drop rows with missing critical values\n",
        "# Capture initial rows for reporting\n",
        "initial_rows_before_drop = len(df)\n",
        "df.dropna(subset=critical_cols, inplace=True)\n",
        "rows_dropped = initial_rows_before_drop - len(df)\n",
        "\n",
        "if rows_dropped > 0:\n",
        "    print(f\"\\nDropped {rows_dropped} rows due to missing critical values in {critical_cols}.\")\n",
        "else:\n",
        "    print(\"\\nNo rows dropped. Critical columns have no missing values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill missing values  for optional columns\n",
        "df['salary_local'] = df['salary_local'].fillna('Not Provided')\n",
        "df['benefits_score'] = df['benefits_score'].fillna(df['benefits_score'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling Missing Values – Final Summary\n",
        "\n",
        " **Findings**\n",
        "- Only one column had missing values: `salary_local` (15,000 rows).\n",
        "- All other 19 columns had **0 missing values**.\n",
        "\n",
        " **Action Taken**\n",
        "- We filled missing `salary_local` values with `\"Not Provided\"`.\n",
        "- No rows were dropped since critical columns (`job_id`, `job_title`, `salary_usd`) had no missing data.\n",
        "\n",
        " **Result**\n",
        "- Total rows: **30,000**\n",
        "- Total columns: **20**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 3: Data Cleaning\n",
        "\n",
        "##  Objective\n",
        "Ensure the dataset is **clean, standardized, and analysis-ready** by removing inconsistencies and formatting errors.\n",
        "\n",
        "## 🔍 What We'll Do\n",
        "1. **Standardize column names**\n",
        "   - Make all lowercase\n",
        "   - Replace spaces with underscores\n",
        "\n",
        "2. **Clean text fields**\n",
        "   - Trim extra spaces\n",
        "   - Ensure consistent capitalization for `job_title`, `company_name`, `industry`\n",
        "\n",
        "3. **Validate date columns**\n",
        "   - Convert `posting_date` & `application_deadline` to datetime format\n",
        "\n",
        "4. **Check salary fields**\n",
        "   - Ensure `salary_usd` is numeric\n",
        "   - Keep `salary_local` as string since some values are \"Not Provided\"\n",
        "\n",
        "5. **Save cleaned dataset**\n",
        "   - File name: `ai_jobs_cleaned_v2.csv`\n",
        "\n",
        "##  Expected Output\n",
        "- Dataset with **clean, standardized column names**\n",
        "- Text fields formatted consistently\n",
        "- Dates properly converted\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No duplicate rows found based on 'job_id'.\n",
            "No unrealistic salary_usd values found within range (1 to 1,000,000 USD).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "job_id                object\n",
              "job_title             object\n",
              "salary_usd             int64\n",
              "salary_currency       object\n",
              "salary_local          object\n",
              "experience_level      object\n",
              "employment_type       object\n",
              "company_location      object\n",
              "company_size          object\n",
              "employee_residence    object\n",
              "dtype: object"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standardize column names\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Clean text fields: strip spaces and standardize capitalization\n",
        "text_cols = ['job_title', 'company_name', 'industry', 'salary_currency',\n",
        "             'experience_level', 'employment_type', 'company_location',\n",
        "             'company_size', 'employee_residence'] \n",
        "for col in text_cols:\n",
        "    if col in df.columns and df[col].dtype == 'object': \n",
        "        df[col] = df[col].astype(str).str.strip().str.title()\n",
        "\n",
        "\n",
        "# Validate and convert date columns\n",
        "df[\"posting_date\"] = pd.to_datetime(df[\"posting_date\"], errors='coerce')\n",
        "df[\"application_deadline\"] = pd.to_datetime(df[\"application_deadline\"], errors='coerce')\n",
        "\n",
        "\n",
        "# Ensure salary_usd is numeric\n",
        "df['salary_usd'] = pd.to_numeric(df['salary_usd'], errors='coerce')\n",
        "\n",
        "# Remove duplicates based on 'job_id' (assuming 'job_id' is unique identifier)\n",
        "initial_rows_before_dedupe = len(df)\n",
        "df.drop_duplicates(subset='job_id', inplace=True)\n",
        "duplicates_removed = initial_rows_before_dedupe - len(df)\n",
        "if duplicates_removed > 0:\n",
        "    print(f\"Removed {duplicates_removed} duplicate rows based on 'job_id'.\")\n",
        "else:\n",
        "    print(\"No duplicate rows found based on 'job_id'.\")\n",
        "\n",
        "# Validate salary ranges - remove highly unrealistic salaries\n",
        "initial_rows_before_salary_check = len(df)\n",
        "df = df[(df['salary_usd'] >= 1) & (df['salary_usd'] <= 1000000)].copy() \n",
        "salaries_out_of_range = initial_rows_before_salary_check - len(df)\n",
        "if salaries_out_of_range > 0:\n",
        "    print(f\"Removed {salaries_out_of_range} rows with unrealistic 'salary_usd' values (<1 or >1M).\")\n",
        "else:\n",
        "    print(\"No unrealistic salary_usd values found within range (1 to 1,000,000 USD).\")\n",
        "\n",
        "# Quick check columns and data types after cleaning\n",
        "df.dtypes.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>salary_usd</th>\n",
              "      <th>salary_currency</th>\n",
              "      <th>salary_local</th>\n",
              "      <th>experience_level</th>\n",
              "      <th>employment_type</th>\n",
              "      <th>company_location</th>\n",
              "      <th>company_size</th>\n",
              "      <th>employee_residence</th>\n",
              "      <th>remote_ratio</th>\n",
              "      <th>required_skills</th>\n",
              "      <th>education_required</th>\n",
              "      <th>years_experience</th>\n",
              "      <th>industry</th>\n",
              "      <th>posting_date</th>\n",
              "      <th>application_deadline</th>\n",
              "      <th>job_description_length</th>\n",
              "      <th>benefits_score</th>\n",
              "      <th>company_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI00001</td>\n",
              "      <td>Ai Research Scientist</td>\n",
              "      <td>90376</td>\n",
              "      <td>Usd</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Se</td>\n",
              "      <td>Ct</td>\n",
              "      <td>China</td>\n",
              "      <td>M</td>\n",
              "      <td>China</td>\n",
              "      <td>50</td>\n",
              "      <td>Tableau, PyTorch, Kubernetes, Linux, NLP</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>9</td>\n",
              "      <td>Automotive</td>\n",
              "      <td>2024-10-18</td>\n",
              "      <td>2024-11-07</td>\n",
              "      <td>1076</td>\n",
              "      <td>5.9</td>\n",
              "      <td>Smart Analytics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AI00002</td>\n",
              "      <td>Ai Software Engineer</td>\n",
              "      <td>61895</td>\n",
              "      <td>Usd</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>En</td>\n",
              "      <td>Ct</td>\n",
              "      <td>Canada</td>\n",
              "      <td>M</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>100</td>\n",
              "      <td>Deep Learning, AWS, Mathematics, Python, Docker</td>\n",
              "      <td>Master</td>\n",
              "      <td>1</td>\n",
              "      <td>Media</td>\n",
              "      <td>2024-11-20</td>\n",
              "      <td>2025-01-11</td>\n",
              "      <td>1268</td>\n",
              "      <td>5.2</td>\n",
              "      <td>Techcorp Inc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI00003</td>\n",
              "      <td>Ai Specialist</td>\n",
              "      <td>152626</td>\n",
              "      <td>Usd</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Mi</td>\n",
              "      <td>Fl</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>L</td>\n",
              "      <td>South Korea</td>\n",
              "      <td>0</td>\n",
              "      <td>Kubernetes, Deep Learning, Java, Hadoop, NLP</td>\n",
              "      <td>Associate</td>\n",
              "      <td>2</td>\n",
              "      <td>Education</td>\n",
              "      <td>2025-03-18</td>\n",
              "      <td>2025-04-07</td>\n",
              "      <td>1974</td>\n",
              "      <td>9.4</td>\n",
              "      <td>Autonomous Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI00004</td>\n",
              "      <td>Nlp Engineer</td>\n",
              "      <td>80215</td>\n",
              "      <td>Usd</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Se</td>\n",
              "      <td>Fl</td>\n",
              "      <td>India</td>\n",
              "      <td>M</td>\n",
              "      <td>India</td>\n",
              "      <td>50</td>\n",
              "      <td>Scala, SQL, Linux, Python</td>\n",
              "      <td>PhD</td>\n",
              "      <td>7</td>\n",
              "      <td>Consulting</td>\n",
              "      <td>2024-12-23</td>\n",
              "      <td>2025-02-24</td>\n",
              "      <td>1345</td>\n",
              "      <td>8.6</td>\n",
              "      <td>Future Systems</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI00005</td>\n",
              "      <td>Ai Consultant</td>\n",
              "      <td>54624</td>\n",
              "      <td>Eur</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>En</td>\n",
              "      <td>Pt</td>\n",
              "      <td>France</td>\n",
              "      <td>S</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>100</td>\n",
              "      <td>MLOps, Java, Tableau, Python</td>\n",
              "      <td>Master</td>\n",
              "      <td>0</td>\n",
              "      <td>Media</td>\n",
              "      <td>2025-04-15</td>\n",
              "      <td>2025-06-23</td>\n",
              "      <td>1989</td>\n",
              "      <td>6.6</td>\n",
              "      <td>Advanced Robotics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    job_id              job_title  salary_usd salary_currency  salary_local  \\\n",
              "0  AI00001  Ai Research Scientist       90376             Usd  Not Provided   \n",
              "1  AI00002   Ai Software Engineer       61895             Usd  Not Provided   \n",
              "2  AI00003          Ai Specialist      152626             Usd  Not Provided   \n",
              "3  AI00004           Nlp Engineer       80215             Usd  Not Provided   \n",
              "4  AI00005          Ai Consultant       54624             Eur  Not Provided   \n",
              "\n",
              "  experience_level employment_type company_location company_size  \\\n",
              "0               Se              Ct            China            M   \n",
              "1               En              Ct           Canada            M   \n",
              "2               Mi              Fl      Switzerland            L   \n",
              "3               Se              Fl            India            M   \n",
              "4               En              Pt           France            S   \n",
              "\n",
              "  employee_residence  remote_ratio  \\\n",
              "0              China            50   \n",
              "1            Ireland           100   \n",
              "2        South Korea             0   \n",
              "3              India            50   \n",
              "4          Singapore           100   \n",
              "\n",
              "                                   required_skills education_required  \\\n",
              "0         Tableau, PyTorch, Kubernetes, Linux, NLP           Bachelor   \n",
              "1  Deep Learning, AWS, Mathematics, Python, Docker             Master   \n",
              "2     Kubernetes, Deep Learning, Java, Hadoop, NLP          Associate   \n",
              "3                        Scala, SQL, Linux, Python                PhD   \n",
              "4                     MLOps, Java, Tableau, Python             Master   \n",
              "\n",
              "   years_experience    industry posting_date application_deadline  \\\n",
              "0                 9  Automotive   2024-10-18           2024-11-07   \n",
              "1                 1       Media   2024-11-20           2025-01-11   \n",
              "2                 2   Education   2025-03-18           2025-04-07   \n",
              "3                 7  Consulting   2024-12-23           2025-02-24   \n",
              "4                 0       Media   2025-04-15           2025-06-23   \n",
              "\n",
              "   job_description_length  benefits_score       company_name  \n",
              "0                    1076             5.9    Smart Analytics  \n",
              "1                    1268             5.2       Techcorp Inc  \n",
              "2                    1974             9.4    Autonomous Tech  \n",
              "3                    1345             8.6     Future Systems  \n",
              "4                    1989             6.6  Advanced Robotics  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 4: Data Transformation\n",
        "\n",
        "##  Objective\n",
        "Make the dataset analysis-friendly by creating new calculated fields and ensuring all formats are correct.\n",
        "\n",
        "##  Planned Transformations\n",
        "1. **Create `salary_category`**\n",
        "   - Low (< $50,000)\n",
        "   - Mid ($50,000–100,000)\n",
        "   - High (> $100,000)\n",
        "\n",
        "2. **Create `remote_status`**\n",
        "   - From `remote_ratio`:\n",
        "     - 0 → Onsite\n",
        "     - 50 → Hybrid\n",
        "     - 100 → Fully Remote\n",
        "\n",
        "3. **Extract date parts**\n",
        "   - From `posting_date` → new columns: `posting_year`, `posting_month`\n",
        "\n",
        "4. **Reorder columns**\n",
        "   - Place the most important ones (`job_id`, `job_title`, `salary_usd`, `salary_category`, `remote_status`) up front\n",
        "\n",
        "## ✅ Expected Output\n",
        "- A richer dataset with **categorical features** and **date insights**\n",
        "- All fields ready for EDA and dashboarding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Columns reordered.\n",
            "Final dataset shape: (15000, 24)\n",
            "Final columns: ['job_id', 'job_title', 'salary_usd', 'salary_category', 'remote_status', 'experience_level', 'employment_type', 'company_location', 'employee_residence', 'company_name', 'company_size', 'remote_ratio', 'salary_currency', 'salary_local', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'posting_year', 'posting_month', 'application_deadline', 'job_description_length', 'benefits_score']\n"
          ]
        }
      ],
      "source": [
        "# Create `salary_category\n",
        "def categorize_salary(salary):\n",
        "    if salary < 50000:\n",
        "        return 'Low'\n",
        "    elif 50000 <= salary <= 100000:\n",
        "        return 'Mid'\n",
        "    else: # salary > 100000\n",
        "        return 'High'\n",
        "\n",
        "df['salary_category'] = df['salary_usd'].apply(categorize_salary)\n",
        "df['salary_category'].value_counts()\n",
        "\n",
        "# Create `remote_status` from `remote_ratio` \n",
        "def map_remote_ratio(ratio):\n",
        "    if ratio == 0:\n",
        "        return 'On-site'\n",
        "    elif ratio == 50:\n",
        "        return 'Hybrid'\n",
        "    elif ratio == 100:\n",
        "        return 'Fully Remote'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "df['remote_status'] = df['remote_ratio'].apply(map_remote_ratio)\n",
        "df['remote_status'].value_counts()\n",
        "\n",
        "\n",
        "#  Extract date parts from `posting_date` \n",
        "# Ensure 'posting_date' is datetime (should be from previous cleaning step)\n",
        "df['posting_year'] = df['posting_date'].dt.year\n",
        "df['posting_month'] = df['posting_date'].dt.month\n",
        "\n",
        "# --- 4. Reorder columns ---\n",
        "# Define the desired order, putting important columns first\n",
        "desired_column_order = [\n",
        "    'job_id',\n",
        "    'job_title',\n",
        "    'salary_usd',\n",
        "    'salary_category',\n",
        "    'remote_status',\n",
        "    'experience_level',\n",
        "    'employment_type',\n",
        "    'company_location',\n",
        "    'employee_residence',\n",
        "    'company_name',\n",
        "    'company_size',\n",
        "    'remote_ratio', \n",
        "    'salary_currency',\n",
        "    'salary_local', \n",
        "    'required_skills',\n",
        "    'education_required',\n",
        "    'years_experience',\n",
        "    'industry',\n",
        "    'posting_date',\n",
        "    'posting_year',\n",
        "    'posting_month',\n",
        "    'application_deadline',\n",
        "    'job_description_length',\n",
        "    'benefits_score'\n",
        "]\n",
        "\n",
        "# Get columns that exist in our DataFrame and are in the desired order\n",
        "final_columns = [col for col in desired_column_order if col in df.columns]\n",
        "df_final = df[final_columns].copy() \n",
        "\n",
        "print(\"\\nColumns reordered.\")\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "print(f\"Final columns: {df_final.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Cleaned Data\n",
        " \n",
        " The final step in the ETL process is to save the cleaned and transformed DataFrame. This ensures that the prepared data can be easily accessed for subsequent analysis phases (Exploratory Data Analysis and Machine Learning Modeling) without needing to re-run the entire cleaning script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df_final.to_csv(\"../data/inputs/cleaned/ai_job_cleaned_dataset.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
