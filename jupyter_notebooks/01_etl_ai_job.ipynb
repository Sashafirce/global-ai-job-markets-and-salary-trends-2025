{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global AI Job Market and Salary Trends 2025: ETL Pipeline\n",
        "\n",
        " This notebook outlines the complete Extract, Transform, Load (ETL) process for the Global AI Job Market and Salary Trends dataset. The goal is to prepare a clean, consistent, and analysis-ready dataset for further exploration and dashboarding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set global display options for pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Define standard paths (adjust if your notebook is in a different directory relative to 'data')\n",
        "RAW_DATA_PATH = \"../data/inputs/raw/\"\n",
        "PROCESSED_DATA_PATH = \"../data/inputs/raw/\"\n",
        "FINAL_DATA_PATH = \"../data/inputs/cleaned/\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(FINAL_DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 1: Data Extraction & Combination\n",
        "\n",
        "## Objective\n",
        " Merge the two raw CSV files (ai_job_dataset.csv and ai_job_dataset1.csv) into a single consolidated dataset, while handling differences in columns.\n",
        "\n",
        "## Datasets\n",
        " - **ai_job_dataset.csv** - Part 1 (19 columns, no `salary_local`)\n",
        " - **ai_job_dataset1.csv** - Part 2 (20 columns, includes `salary_local`)\n",
        "\n",
        "## Process\n",
        " 1. Load both CSV files with pandas.\n",
        " 2. Align their columns by adding any missing columns (like `salary_local`) with `NaN` values in the first dataset.\n",
        " 3. Concatenate the datasets into one.\n",
        " 4. Display initial statistics (`df.describe()`) of the combined data.\n",
        " 5. Save the combined dataset as `ai_jobs_combined.csv` in the `processed` directory for the next ETL steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset created. Total rows: 30000\n",
            "Final columns: ['job_id', 'job_title', 'salary_usd', 'salary_currency', 'salary_local', 'experience_level', 'employment_type', 'company_location', 'company_size', 'employee_residence', 'remote_ratio', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'application_deadline', 'job_description_length', 'benefits_score', 'company_name']\n",
            "\n",
            "--- Initial Statistics of Combined Data (df.describe()) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>salary_usd</th>\n",
              "      <th>salary_currency</th>\n",
              "      <th>salary_local</th>\n",
              "      <th>experience_level</th>\n",
              "      <th>employment_type</th>\n",
              "      <th>company_location</th>\n",
              "      <th>company_size</th>\n",
              "      <th>employee_residence</th>\n",
              "      <th>remote_ratio</th>\n",
              "      <th>required_skills</th>\n",
              "      <th>education_required</th>\n",
              "      <th>years_experience</th>\n",
              "      <th>industry</th>\n",
              "      <th>posting_date</th>\n",
              "      <th>application_deadline</th>\n",
              "      <th>job_description_length</th>\n",
              "      <th>benefits_score</th>\n",
              "      <th>company_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000.000000</td>\n",
              "      <td>30000</td>\n",
              "      <td>15000.0</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000.000000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000.000000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000</td>\n",
              "      <td>30000.000000</td>\n",
              "      <td>30000.000000</td>\n",
              "      <td>30000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>15000</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>14458.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25702</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>486</td>\n",
              "      <td>543</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>AI00001</td>\n",
              "      <td>Machine Learning Engineer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>USD</td>\n",
              "      <td>131854.0</td>\n",
              "      <td>EX</td>\n",
              "      <td>CT</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>L</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Python, TensorFlow, PyTorch</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consulting</td>\n",
              "      <td>2024-08-25</td>\n",
              "      <td>2024-10-07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TechCorp Inc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2</td>\n",
              "      <td>1596</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19410</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7603</td>\n",
              "      <td>7562</td>\n",
              "      <td>1565</td>\n",
              "      <td>10085</td>\n",
              "      <td>1417</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25</td>\n",
              "      <td>7652</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2041</td>\n",
              "      <td>85</td>\n",
              "      <td>87</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>118670.451700</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49.840000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.309433</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1502.083667</td>\n",
              "      <td>7.501907</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>62229.977054</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.829278</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.572413</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>575.418018</td>\n",
              "      <td>1.447517</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16621.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72575.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>6.300000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103206.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1512.000000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150921.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>8.800000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>410273.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2499.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         job_id                  job_title     salary_usd salary_currency  salary_local experience_level employment_type company_location company_size employee_residence  remote_ratio              required_skills education_required  years_experience    industry posting_date application_deadline  job_description_length  benefits_score  company_name\n",
              "count     30000                      30000   30000.000000           30000       15000.0            30000           30000            30000        30000              30000  30000.000000                        30000              30000      30000.000000       30000        30000                30000            30000.000000    30000.000000         30000\n",
              "unique    15000                         20            NaN               8       14458.0                4               4               20            3                 50           NaN                        25702                  4               NaN          15          486                  543                     NaN             NaN            16\n",
              "top     AI00001  Machine Learning Engineer            NaN             USD      131854.0               EX              CT      Switzerland            L        Switzerland           NaN  Python, TensorFlow, PyTorch           Bachelor               NaN  Consulting   2024-08-25           2024-10-07                     NaN             NaN  TechCorp Inc\n",
              "freq          2                       1596            NaN           19410           3.0             7603            7562             1565        10085               1417           NaN                           25               7652               NaN        2041           85                   87                     NaN             NaN          1920\n",
              "mean        NaN                        NaN  118670.451700             NaN           NaN              NaN             NaN              NaN          NaN                NaN     49.840000                          NaN                NaN          6.309433         NaN          NaN                  NaN             1502.083667        7.501907           NaN\n",
              "std         NaN                        NaN   62229.977054             NaN           NaN              NaN             NaN              NaN          NaN                NaN     40.829278                          NaN                NaN          5.572413         NaN          NaN                  NaN              575.418018        1.447517           NaN\n",
              "min         NaN                        NaN   16621.000000             NaN           NaN              NaN             NaN              NaN          NaN                NaN      0.000000                          NaN                NaN          0.000000         NaN          NaN                  NaN              500.000000        5.000000           NaN\n",
              "25%         NaN                        NaN   72575.750000             NaN           NaN              NaN             NaN              NaN          NaN                NaN      0.000000                          NaN                NaN          2.000000         NaN          NaN                  NaN             1001.000000        6.300000           NaN\n",
              "50%         NaN                        NaN  103206.500000             NaN           NaN              NaN             NaN              NaN          NaN                NaN     50.000000                          NaN                NaN          5.000000         NaN          NaN                  NaN             1512.000000        7.500000           NaN\n",
              "75%         NaN                        NaN  150921.750000             NaN           NaN              NaN             NaN              NaN          NaN                NaN    100.000000                          NaN                NaN         10.000000         NaN          NaN                  NaN             1997.000000        8.800000           NaN\n",
              "max         NaN                        NaN  410273.000000             NaN           NaN              NaN             NaN              NaN          NaN                NaN    100.000000                          NaN                NaN         19.000000         NaN          NaN                  NaN             2499.000000       10.000000           NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset saved to ../data/inputs/raw/ai_jobs_combined.csv\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df1 = pd.read_csv(os.path.join(RAW_DATA_PATH, \"ai_job_dataset.csv\"))\n",
        "df2 = pd.read_csv(os.path.join(RAW_DATA_PATH, \"ai_job_dataset1.csv\"))\n",
        "\n",
        "# Align the columns of the two dataframes\n",
        "for col in df2.columns:\n",
        "    if col not in df1.columns:\n",
        "        df1[col] = pd.NA # Fill missing columns with NA\n",
        "\n",
        "# Ensure both datasets have same column order\n",
        "df1 = df1[df2.columns]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "print(f\"Combined dataset created. Total rows: {combined_df.shape[0]}\")\n",
        "print(f\"Final columns: {combined_df.columns.tolist()}\")\n",
        "\n",
        "print(\"\\n--- Initial Statistics of Combined Data (df.describe()) ---\")\n",
        "display(combined_df.describe(include='all')) # Include 'all' for mixed types\n",
        "\n",
        "# Save the combined dataset to the processed directory\n",
        "combined_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"ai_jobs_combined.csv\"), index=False)\n",
        "print(f\"Combined dataset saved to {os.path.join(PROCESSED_DATA_PATH, 'ai_jobs_combined.csv')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 2: Handling Missing Values\n",
        "\n",
        "## Objective\n",
        " Address missing values in the combined dataset, ensuring data completeness without unnecessary loss of information.\n",
        "\n",
        "## Focus Areas\n",
        " - **`salary_local`**: This column was introduced from `df2` and is `NaN` for all rows originating from `df1`. We will fill these with a meaningful placeholder.\n",
        " - **Critical Columns**: Identify and handle missing values in core columns (`job_id`, `job_title`, `salary_usd`). If these are missing, it might indicate corrupted records.\n",
        "\n",
        "## Process\n",
        " 1. Load the `ai_jobs_combined.csv` dataset.\n",
        " 2. Fill `NaN` values in `salary_local` with 'Not Provided'.\n",
        " 3. Check for `NaN` values in critical columns and report/handle them (e.g., drop rows if truly unrecoverable for core identifiers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Missing Value Handling ---\n",
            "Missing values before handling:\n",
            " salary_local    15000\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the combined dataset\n",
        "df = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"ai_jobs_combined.csv\"), dtype={'salary_local': str}) # Load salary_local as string to avoid type issues\n",
        "\n",
        "print(\"--- Missing Value Handling ---\")\n",
        "print(\"Missing values before handling:\\n\", df.isnull().sum()[df.isnull().sum() > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Handle `salary_local` missing values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "'salary_local' NaN values filled with 'Not Provided'.\n"
          ]
        }
      ],
      "source": [
        "# Fill NaN values in 'salary_local' with 'Not Provided' to retain rows\n",
        "df['salary_local'] = df['salary_local'].fillna('Not Provided')\n",
        "print(\"\\n'salary_local' NaN values filled with 'Not Provided'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No missing values found in critical columns.\n",
            "\n",
            "Missing values after handling:\n",
            " Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "#  Check for and handle critical column missing values \n",
        "critical_columns = ['job_id', 'job_title', 'salary_usd']\n",
        "missing_critical = df[critical_columns].isnull().sum()\n",
        "if missing_critical.sum() > 0:\n",
        "    print(\"\\nWARNING: Missing values found in critical columns:\")\n",
        "    print(missing_critical[missing_critical > 0])\n",
        "\n",
        "    initial_rows = df.shape[0]\n",
        "    df.dropna(subset=critical_columns, inplace=True)\n",
        "    rows_dropped = initial_rows - df.shape[0]\n",
        "    if rows_dropped > 0:\n",
        "        print(f\"{rows_dropped} rows dropped due to missing critical information.\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found in critical columns.\")\n",
        "\n",
        "print(\"\\nMissing values after handling:\\n\", df.isnull().sum()[df.isnull().sum() > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 3: Data Cleaning & Quality\n",
        "\n",
        "## Objective\n",
        " Refine data quality by standardizing formats, removing inconsistencies, and handling outliers.\n",
        "\n",
        "## Process\n",
        " 1. **Column Names**: Standardize all column names to lowercase with underscores.\n",
        " 2. **Text Fields**: Clean and standardize text fields (`job_title`, `company_name`, `company_location`, `employee_residence`, `industry`, `required_skills`) to Title Case and strip extra spaces.\n",
        " 3. **Date Conversion**: Convert `posting_date` and `application_deadline` to datetime objects.\n",
        " 4. **Data Types**: Ensure `salary_usd` is numeric.\n",
        " 5. **Duplicate Removal**: Remove duplicate rows based on `job_id`.\n",
        " 6. **Outlier Handling (`salary_usd`)**: Remove unrealistic salary values to ensure data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Cleaning & Quality ---\n",
            "Column names standardized.\n",
            "New columns:  ['job_id', 'job_title', 'salary_usd', 'salary_currency', 'salary_local', 'experience_level', 'employment_type', 'company_location', 'company_size', 'employee_residence', 'remote_ratio', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'application_deadline', 'job_description_length', 'benefits_score', 'company_name']\n",
            "Text fields standardized (stripped, title cased). \n",
            "Date columns converted to datetime objects.\n",
            "'salary_usd' ensured as numeric, NaNs filled with median.\n",
            "15000 duplicate rows removed based on 'job_id'.\n",
            "0 rows removed due to 'salary_usd' outliers (outside $1000-$1000000).\n",
            "Data cleaning and quality checks completed.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data Cleaning & Quality ---\")\n",
        "\n",
        "# Standardize Column Names\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "print(\"Column names standardized.\")\n",
        "print(\"New columns: \", df.columns.tolist())\n",
        "\n",
        "# Clean and Standardize Text Fields\n",
        "text_cols = ['job_title', 'company_name', 'company_location', 'employee_residence', 'industry', 'required_skills']\n",
        "for col in text_cols:\n",
        "    if col in df.columns:\n",
        "        # Fill NaN with empty string before applying string methods to avoid errors\n",
        "        df[col] = df[col].fillna('').astype(str).str.strip().str.title()\n",
        "print(\"Text fields standardized (stripped, title cased). \")\n",
        "\n",
        "# Convert Date Columns \n",
        "date_cols = ['posting_date', 'application_deadline']\n",
        "for col in date_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "print(\"Date columns converted to datetime objects.\")\n",
        "\n",
        "# Ensure salary_usd is numeric\n",
        "df['salary_usd'] = pd.to_numeric(df['salary_usd'], errors='coerce')\n",
        "# After coercing, if any NaNs are created, fill them to maintain data integrity\n",
        "# A strategy like median or mean could be used, or mark as 'unknown' if not critical for analysis\n",
        "df['salary_usd'].fillna(df['salary_usd'].median(), inplace=True) # Impute with median for numerical consistency\n",
        "print(\"'salary_usd' ensured as numeric, NaNs filled with median.\")\n",
        "\n",
        "# Remove Duplicate Rows based on job_id\n",
        "initial_rows = df.shape[0]\n",
        "df.drop_duplicates(subset=['job_id'], inplace=True)\n",
        "rows_dropped = initial_rows - df.shape[0]\n",
        "if rows_dropped > 0:\n",
        "    print(f\"{rows_dropped} duplicate rows removed based on 'job_id'.\")\n",
        "else:\n",
        "    print(\"No duplicate rows found based on 'job_id'.\")\n",
        "\n",
        "# Outlier Handling for salary_usd (remove unrealistic values) \n",
        "salary_min = 1000  # Minimum plausible salary (e.g., to filter out data entry errors or very low part-time gigs if not relevant)\n",
        "salary_max = 1_000_000 # Maximum plausible salary for general AI roles\n",
        "\n",
        "initial_rows = df.shape[0]\n",
        "df = df[(df['salary_usd'] >= salary_min) & (df['salary_usd'] <= salary_max)]\n",
        "rows_removed_outliers = initial_rows - df.shape[0]\n",
        "print(f\"{rows_removed_outliers} rows removed due to 'salary_usd' outliers (outside ${salary_min}-${salary_max}).\")\n",
        "\n",
        "print(\"Data cleaning and quality checks completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Step 4: Data Transformation & Feature Engineering\n",
        "\n",
        "## Objective\n",
        " Create new, insightful features and standardize existing categorical data to enhance readability and analytical capabilities.\n",
        "\n",
        "## Process\n",
        " 1. **`salary_category`**: Categorize `salary_usd` into 'Low', 'Mid', 'High' for readability.\n",
        " 2. **`remote_status`**: Derive remote work status from `remote_ratio`.\n",
        " 3. **Time Features**: Extract `posting_year` and `posting_month` from `posting_date`.\n",
        " 4. **`experience_level` Mapping**: Map abbreviations to full, descriptive names.\n",
        " 5. **`employment_type` Mapping**: Standardize employment type names.\n",
        " 6. **`company_size` Mapping**: Standardize company size names and set a categorical order.\n",
        " 7. **Column Reordering**: Reorder columns for logical presentation.\n",
        " 8. **Save Final Data**: Save the fully transformed dataset as `ai_jobs_final.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Transformation & Feature Engineering ---\n",
            "'salary_category' created: Low (<$50k), Mid ($50k-$100k), High (>$100k).\n",
            "Salary Category Distribution:\n",
            " salary_category\n",
            "High    7468\n",
            "Mid     6392\n",
            "Low     1140\n",
            "Name: count, dtype: int64\n",
            "'remote_status' created.\n",
            "'posting_year' and 'posting_month' created.\n",
            "\n",
            "Original unique 'experience_level': ['SE' 'EN' 'MI' 'EX']\n",
            "Experience levels mapped to full names.\n",
            "Updated unique 'experience_level': ['Senior' 'Entry-level' 'Mid-level' 'Executive']\n",
            "\n",
            "Original unique 'employment_type': ['CT' 'FL' 'PT' 'FT']\n",
            "Employment types standardized.\n",
            "Updated unique 'employment_type': ['Contract' 'Freelance' 'Part-time' 'Full-time']\n",
            "\n",
            "Original unique 'company_size': ['M' 'L' 'S']\n",
            "Company sizes standardized and ordered.\n",
            "Updated unique 'company_size': ['Medium (50-249 employees)', 'Large (250+ employees)', 'Small (<50 employees)']\n",
            "Categories (3, object): ['Small (<50 employees)' < 'Medium (50-249 employees)' < 'Large (250+ employees)']\n",
            "Columns reordered for final dataset.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data Transformation & Feature Engineering ---\")\n",
        "\n",
        "# Categorize Salary (USD) into 'salary_category'\n",
        "def categorize_salary(salary):\n",
        "    if salary < 50000:\n",
        "        return 'Low'\n",
        "    elif 50000 <= salary <= 100000:\n",
        "        return 'Mid'\n",
        "    else: # salary > 100000\n",
        "        return 'High'\n",
        "\n",
        "df['salary_category'] = df['salary_usd'].apply(categorize_salary)\n",
        "print(\"'salary_category' created: Low (<$50k), Mid ($50k-$100k), High (>$100k).\")\n",
        "print(\"Salary Category Distribution:\\n\", df['salary_category'].value_counts())\n",
        "\n",
        "# Create 'remote_status' from 'remote_ratio' \n",
        "def get_remote_status(ratio):\n",
        "    if ratio == 100:\n",
        "        return 'Fully Remote'\n",
        "    elif ratio == 0:\n",
        "        return 'On-site'\n",
        "    else:\n",
        "        return 'Hybrid'\n",
        "\n",
        "df['remote_status'] = df['remote_ratio'].apply(get_remote_status)\n",
        "print(\"'remote_status' created.\")\n",
        "\n",
        "# Extract Time Features from 'posting_date'\n",
        "df['posting_year'] = df['posting_date'].dt.year\n",
        "df['posting_month'] = df['posting_date'].dt.month\n",
        "print(\"'posting_year' and 'posting_month' created.\")\n",
        "\n",
        "# Map 'experience_level' abbreviations to full names\n",
        "print(\"\\nOriginal unique 'experience_level':\", df['experience_level'].unique())\n",
        "experience_level_mapping = {\n",
        "    'EN': 'Entry-level',\n",
        "    'MI': 'Mid-level',\n",
        "    'SE': 'Senior',\n",
        "    'EX': 'Executive'\n",
        "}\n",
        "df['experience_level'] = df['experience_level'].apply(lambda x: experience_level_mapping.get(x, x))\n",
        "print(\"Experience levels mapped to full names.\")\n",
        "print(\"Updated unique 'experience_level':\", df['experience_level'].unique())\n",
        "\n",
        "# Map 'employment_type' for standardization \n",
        "print(\"\\nOriginal unique 'employment_type':\", df['employment_type'].unique())\n",
        "employment_type_mapping = {\n",
        "    'FT': 'Full-time',\n",
        "    'PT': 'Part-time', # Corrected to lowercase 'part-time' for consistency\n",
        "    'CT': 'Contract',\n",
        "    'FL': 'Freelance'\n",
        "}\n",
        "df['employment_type'] = df['employment_type'].apply(lambda x: employment_type_mapping.get(x, x))\n",
        "print(\"Employment types standardized.\")\n",
        "print(\"Updated unique 'employment_type':\", df['employment_type'].unique())\n",
        "\n",
        "# Map 'company_size' for standardization and ordering \n",
        "print(\"\\nOriginal unique 'company_size':\", df['company_size'].unique())\n",
        "company_size_mapping = {\n",
        "    'S': 'Small (<50 employees)',\n",
        "    'M': 'Medium (50-249 employees)',\n",
        "    'L': 'Large (250+ employees)'\n",
        "}\n",
        "df['company_size'] = df['company_size'].apply(lambda x: company_size_mapping.get(x, x))\n",
        "\n",
        "# Define a categorical order for proper visualization later\n",
        "company_size_order = [\n",
        "    'Small (<50 employees)',\n",
        "    'Medium (50-249 employees)',\n",
        "    'Large (250+ employees)'\n",
        "]\n",
        "# Convert to a categorical type with a defined order\n",
        "df['company_size'] = pd.Categorical(df['company_size'], categories=company_size_order, ordered=True)\n",
        "print(\"Company sizes standardized and ordered.\")\n",
        "print(\"Updated unique 'company_size':\", df['company_size'].unique())\n",
        "\n",
        "# Reorder Columns for Logical Presentation \n",
        "final_columns_order = [\n",
        "    'job_id', 'job_title', 'company_name', 'company_location', 'employee_residence',\n",
        "    'industry', 'experience_level', 'employment_type', 'company_size',\n",
        "    'remote_ratio', 'remote_status',\n",
        "    'salary_usd', 'salary_currency', 'salary_local', 'salary_category',\n",
        "    'required_skills', 'education_required', 'years_experience',\n",
        "    'posting_date', 'posting_year', 'posting_month', 'application_deadline',\n",
        "    'job_description_length', 'benefits_score'\n",
        "]\n",
        "\n",
        "# Ensure all columns in df are in the final_columns_order or append new ones\n",
        "existing_cols = df.columns.tolist()\n",
        "final_columns_order = [col for col in final_columns_order if col in existing_cols] # Only include existing columns\n",
        "for col in existing_cols:\n",
        "    if col not in final_columns_order:\n",
        "        final_columns_order.append(col) # Add any new/unexpected columns at the end\n",
        "\n",
        "df = df[final_columns_order]\n",
        "print(\"Columns reordered for final dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>company_name</th>\n",
              "      <th>company_location</th>\n",
              "      <th>employee_residence</th>\n",
              "      <th>industry</th>\n",
              "      <th>experience_level</th>\n",
              "      <th>employment_type</th>\n",
              "      <th>company_size</th>\n",
              "      <th>remote_ratio</th>\n",
              "      <th>remote_status</th>\n",
              "      <th>salary_usd</th>\n",
              "      <th>salary_currency</th>\n",
              "      <th>salary_local</th>\n",
              "      <th>salary_category</th>\n",
              "      <th>required_skills</th>\n",
              "      <th>education_required</th>\n",
              "      <th>years_experience</th>\n",
              "      <th>posting_date</th>\n",
              "      <th>posting_year</th>\n",
              "      <th>posting_month</th>\n",
              "      <th>application_deadline</th>\n",
              "      <th>job_description_length</th>\n",
              "      <th>benefits_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI00001</td>\n",
              "      <td>Ai Research Scientist</td>\n",
              "      <td>Smart Analytics</td>\n",
              "      <td>China</td>\n",
              "      <td>China</td>\n",
              "      <td>Automotive</td>\n",
              "      <td>Senior</td>\n",
              "      <td>Contract</td>\n",
              "      <td>Medium (50-249 employees)</td>\n",
              "      <td>50</td>\n",
              "      <td>Hybrid</td>\n",
              "      <td>90376</td>\n",
              "      <td>USD</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Mid</td>\n",
              "      <td>Tableau, Pytorch, Kubernetes, Linux, Nlp</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>9</td>\n",
              "      <td>2024-10-18</td>\n",
              "      <td>2024</td>\n",
              "      <td>10</td>\n",
              "      <td>2024-11-07</td>\n",
              "      <td>1076</td>\n",
              "      <td>5.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AI00002</td>\n",
              "      <td>Ai Software Engineer</td>\n",
              "      <td>Techcorp Inc</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Media</td>\n",
              "      <td>Entry-level</td>\n",
              "      <td>Contract</td>\n",
              "      <td>Medium (50-249 employees)</td>\n",
              "      <td>100</td>\n",
              "      <td>Fully Remote</td>\n",
              "      <td>61895</td>\n",
              "      <td>USD</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Mid</td>\n",
              "      <td>Deep Learning, Aws, Mathematics, Python, Docker</td>\n",
              "      <td>Master</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-11-20</td>\n",
              "      <td>2024</td>\n",
              "      <td>11</td>\n",
              "      <td>2025-01-11</td>\n",
              "      <td>1268</td>\n",
              "      <td>5.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI00003</td>\n",
              "      <td>Ai Specialist</td>\n",
              "      <td>Autonomous Tech</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>South Korea</td>\n",
              "      <td>Education</td>\n",
              "      <td>Mid-level</td>\n",
              "      <td>Freelance</td>\n",
              "      <td>Large (250+ employees)</td>\n",
              "      <td>0</td>\n",
              "      <td>On-site</td>\n",
              "      <td>152626</td>\n",
              "      <td>USD</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>High</td>\n",
              "      <td>Kubernetes, Deep Learning, Java, Hadoop, Nlp</td>\n",
              "      <td>Associate</td>\n",
              "      <td>2</td>\n",
              "      <td>2025-03-18</td>\n",
              "      <td>2025</td>\n",
              "      <td>3</td>\n",
              "      <td>2025-04-07</td>\n",
              "      <td>1974</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI00004</td>\n",
              "      <td>Nlp Engineer</td>\n",
              "      <td>Future Systems</td>\n",
              "      <td>India</td>\n",
              "      <td>India</td>\n",
              "      <td>Consulting</td>\n",
              "      <td>Senior</td>\n",
              "      <td>Freelance</td>\n",
              "      <td>Medium (50-249 employees)</td>\n",
              "      <td>50</td>\n",
              "      <td>Hybrid</td>\n",
              "      <td>80215</td>\n",
              "      <td>USD</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Mid</td>\n",
              "      <td>Scala, Sql, Linux, Python</td>\n",
              "      <td>PhD</td>\n",
              "      <td>7</td>\n",
              "      <td>2024-12-23</td>\n",
              "      <td>2024</td>\n",
              "      <td>12</td>\n",
              "      <td>2025-02-24</td>\n",
              "      <td>1345</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI00005</td>\n",
              "      <td>Ai Consultant</td>\n",
              "      <td>Advanced Robotics</td>\n",
              "      <td>France</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>Media</td>\n",
              "      <td>Entry-level</td>\n",
              "      <td>Part-time</td>\n",
              "      <td>Small (&lt;50 employees)</td>\n",
              "      <td>100</td>\n",
              "      <td>Fully Remote</td>\n",
              "      <td>54624</td>\n",
              "      <td>EUR</td>\n",
              "      <td>Not Provided</td>\n",
              "      <td>Mid</td>\n",
              "      <td>Mlops, Java, Tableau, Python</td>\n",
              "      <td>Master</td>\n",
              "      <td>0</td>\n",
              "      <td>2025-04-15</td>\n",
              "      <td>2025</td>\n",
              "      <td>4</td>\n",
              "      <td>2025-06-23</td>\n",
              "      <td>1989</td>\n",
              "      <td>6.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    job_id              job_title       company_name company_location employee_residence    industry experience_level employment_type               company_size  remote_ratio remote_status  salary_usd salary_currency  salary_local salary_category                                  required_skills education_required  years_experience posting_date  posting_year  posting_month application_deadline  job_description_length  benefits_score\n",
              "0  AI00001  Ai Research Scientist    Smart Analytics            China              China  Automotive           Senior        Contract  Medium (50-249 employees)            50        Hybrid       90376             USD  Not Provided             Mid         Tableau, Pytorch, Kubernetes, Linux, Nlp           Bachelor                 9   2024-10-18          2024             10           2024-11-07                    1076             5.9\n",
              "1  AI00002   Ai Software Engineer       Techcorp Inc           Canada            Ireland       Media      Entry-level        Contract  Medium (50-249 employees)           100  Fully Remote       61895             USD  Not Provided             Mid  Deep Learning, Aws, Mathematics, Python, Docker             Master                 1   2024-11-20          2024             11           2025-01-11                    1268             5.2\n",
              "2  AI00003          Ai Specialist    Autonomous Tech      Switzerland        South Korea   Education        Mid-level       Freelance     Large (250+ employees)             0       On-site      152626             USD  Not Provided            High     Kubernetes, Deep Learning, Java, Hadoop, Nlp          Associate                 2   2025-03-18          2025              3           2025-04-07                    1974             9.4\n",
              "3  AI00004           Nlp Engineer     Future Systems            India              India  Consulting           Senior       Freelance  Medium (50-249 employees)            50        Hybrid       80215             USD  Not Provided             Mid                        Scala, Sql, Linux, Python                PhD                 7   2024-12-23          2024             12           2025-02-24                    1345             8.6\n",
              "4  AI00005          Ai Consultant  Advanced Robotics           France          Singapore       Media      Entry-level       Part-time      Small (<50 employees)           100  Fully Remote       54624             EUR  Not Provided             Mid                     Mlops, Java, Tableau, Python             Master                 0   2025-04-15          2025              4           2025-06-23                    1989             6.6"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Cleaned Data\n",
        " \n",
        " The final step in the ETL process is to save the cleaned and transformed DataFrame. This ensures that the prepared data can be easily accessed for subsequent analysis phases (Exploratory Data Analysis and Machine Learning Modeling) without needing to re-run the entire cleaning script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final transformed dataset saved to ../data/inputs/cleaned/ai_jobs_final.csv\n",
            "\n",
            "--- ETL Process Completed ---\n",
            "Final Dataframe Head:\n",
            "     job_id              job_title       company_name company_location employee_residence    industry experience_level employment_type               company_size  remote_ratio remote_status  salary_usd salary_currency  salary_local salary_category                                  required_skills education_required  years_experience posting_date  posting_year  posting_month application_deadline  job_description_length  benefits_score\n",
            "0  AI00001  Ai Research Scientist    Smart Analytics            China              China  Automotive           Senior        Contract  Medium (50-249 employees)            50        Hybrid       90376             USD  Not Provided             Mid         Tableau, Pytorch, Kubernetes, Linux, Nlp           Bachelor                 9   2024-10-18          2024             10           2024-11-07                    1076             5.9\n",
            "1  AI00002   Ai Software Engineer       Techcorp Inc           Canada            Ireland       Media      Entry-level        Contract  Medium (50-249 employees)           100  Fully Remote       61895             USD  Not Provided             Mid  Deep Learning, Aws, Mathematics, Python, Docker             Master                 1   2024-11-20          2024             11           2025-01-11                    1268             5.2\n",
            "2  AI00003          Ai Specialist    Autonomous Tech      Switzerland        South Korea   Education        Mid-level       Freelance     Large (250+ employees)             0       On-site      152626             USD  Not Provided            High     Kubernetes, Deep Learning, Java, Hadoop, Nlp          Associate                 2   2025-03-18          2025              3           2025-04-07                    1974             9.4\n",
            "3  AI00004           Nlp Engineer     Future Systems            India              India  Consulting           Senior       Freelance  Medium (50-249 employees)            50        Hybrid       80215             USD  Not Provided             Mid                        Scala, Sql, Linux, Python                PhD                 7   2024-12-23          2024             12           2025-02-24                    1345             8.6\n",
            "4  AI00005          Ai Consultant  Advanced Robotics           France          Singapore       Media      Entry-level       Part-time      Small (<50 employees)           100  Fully Remote       54624             EUR  Not Provided             Mid                     Mlops, Java, Tableau, Python             Master                 0   2025-04-15          2025              4           2025-06-23                    1989             6.6\n",
            "Final Dataframe Info:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Save the final transformed dataset to CSV\n",
        "df.to_csv(os.path.join(FINAL_DATA_PATH, \"ai_jobs_final.csv\"), index=False)\n",
        "print(f\"Final transformed dataset saved to {os.path.join(FINAL_DATA_PATH, 'ai_jobs_final.csv')}\")\n",
        "\n",
        "print(\"\\n--- ETL Process Completed ---\")\n",
        "print(\"Final Dataframe Head:\\n\", df.head())\n",
        "print(\"Final Dataframe Info:\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 15000 entries, 0 to 14999\n",
            "Data columns (total 24 columns):\n",
            " #   Column                  Non-Null Count  Dtype         \n",
            "---  ------                  --------------  -----         \n",
            " 0   job_id                  15000 non-null  object        \n",
            " 1   job_title               15000 non-null  object        \n",
            " 2   company_name            15000 non-null  object        \n",
            " 3   company_location        15000 non-null  object        \n",
            " 4   employee_residence      15000 non-null  object        \n",
            " 5   industry                15000 non-null  object        \n",
            " 6   experience_level        15000 non-null  object        \n",
            " 7   employment_type         15000 non-null  object        \n",
            " 8   company_size            15000 non-null  category      \n",
            " 9   remote_ratio            15000 non-null  int64         \n",
            " 10  remote_status           15000 non-null  object        \n",
            " 11  salary_usd              15000 non-null  int64         \n",
            " 12  salary_currency         15000 non-null  object        \n",
            " 13  salary_local            15000 non-null  object        \n",
            " 14  salary_category         15000 non-null  object        \n",
            " 15  required_skills         15000 non-null  object        \n",
            " 16  education_required      15000 non-null  object        \n",
            " 17  years_experience        15000 non-null  int64         \n",
            " 18  posting_date            15000 non-null  datetime64[ns]\n",
            " 19  posting_year            15000 non-null  int32         \n",
            " 20  posting_month           15000 non-null  int32         \n",
            " 21  application_deadline    15000 non-null  datetime64[ns]\n",
            " 22  job_description_length  15000 non-null  int64         \n",
            " 23  benefits_score          15000 non-null  float64       \n",
            "dtypes: category(1), datetime64[ns](2), float64(1), int32(2), int64(4), object(14)\n",
            "memory usage: 2.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
